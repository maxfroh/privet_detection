{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/My Drive/Privet/data/pruned\")"
      ],
      "metadata": {
        "id": "NiS_lN7VSiUF",
        "outputId": "8b04a54a-e0ce-43c5-d4f4-9087e03f9c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "id": "NiS_lN7VSiUF",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must not contain a space.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-4145840967.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My\\ Drive/Privet/data/pruned\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmountpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not contain a space.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'VERTEX_PRODUCT'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'COLAB_ENTERPRISE'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not contain a space."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f1da49",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "77f1da49"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from os import PathLike\n",
        "from tqdm import tqdm\n",
        "from operator import itemgetter\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.optim import Optimizer, SGD\n",
        "from torch.optim.lr_scheduler import LRScheduler, StepLR\n",
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "from models.fast_rcnn import FasterRCNNResNet101\n",
        "from data_parsing.dataloader import PrivetDataset\n",
        "from data_parsing.graph_maker import make_graphs_and_vis\n",
        "from torch_references.utils import collate_fn\n",
        "from torch_references.engine import train_one_epoch, evaluate\n",
        "\n",
        "RAND_SEED = 7\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 1e-3\n",
        "OPTIM_MOMENTUM = 0.9\n",
        "OPTIM_WEIGHT_DECAY = 0.0005\n",
        "SCHEDULER_STEP_SIZE = 30\n",
        "SCHEDULER_GAMMA = 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "#  Setup  Functions  #\n",
        "######################\n",
        "\n",
        "\n",
        "def set_seeds():\n",
        "    random.seed(RAND_SEED)\n",
        "    np.random.seed(RAND_SEED)\n",
        "    torch.random.manual_seed(RAND_SEED)\n",
        "    torch.cuda.manual_seed_all(RAND_SEED)\n",
        "\n",
        "\n",
        "def get_model(model: str, num_channels: int) -> torch.nn.Module:\n",
        "    match model:\n",
        "        case \"faster_rcnn\":\n",
        "            return FasterRCNNResNet101(num_channels=num_channels)\n",
        "\n",
        "\n",
        "def get_transforms(train: bool = True):\n",
        "    \"\"\"\n",
        "    Return transforms for the data.\n",
        "    \"\"\"\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms=transforms)\n",
        "\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n"
      ],
      "metadata": {
        "id": "gPhCyoLPSunO"
      },
      "id": "gPhCyoLPSunO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(img_dir: str | PathLike, labels_dir: str | PathLike, channels: str, batch_size: int = BATCH_SIZE, num_folds: int = 1) -> dict[int, tuple[DataLoader, DataLoader, DataLoader]]:\n",
        "    is_multispectral = True if channels == \"all\" else False\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(RAND_SEED)\n",
        "\n",
        "    dataset = PrivetDataset(img_dir=img_dir, labels_dir=labels_dir,\n",
        "                            is_multispectral=is_multispectral)\n",
        "\n",
        "    train_transform = get_transforms(train=True)\n",
        "    test_transform = get_transforms(train=False)\n",
        "\n",
        "    dls: dict[int, tuple[DataLoader, DataLoader, DataLoader]] = {}\n",
        "    idxs = np.random.permutation(range(len(dataset)))\n",
        "\n",
        "    if num_folds > 1:\n",
        "        # 20%\n",
        "        test_idx = len(dataset) - int(len(dataset) * 0.1)\n",
        "        train_idxs = idxs[:test_idx]\n",
        "        test_idxs = idxs[test_idx:]\n",
        "        full_train_data = Subset(dataset=dataset, indices=train_idxs)\n",
        "        test_data = Subset(dataset=dataset, indices=test_idxs)\n",
        "        test_data = DataLoader(\n",
        "            dataset=test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        fold = KFold(n_splits=num_folds, shuffle=True, random_state=RAND_SEED)\n",
        "\n",
        "        splits = fold.split(full_train_data)\n",
        "        for fold, (train, val) in enumerate(splits):\n",
        "            training_data = Subset(dataset, train)\n",
        "            validation_data = Subset(dataset, val)\n",
        "            training_data.transform = train_transform\n",
        "            validation_data.transform = test_transform\n",
        "            training_data = DataLoader(\n",
        "                dataset=training_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "            validation_data = DataLoader(\n",
        "                dataset=validation_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "            dls[fold] = (training_data, validation_data, test_data)\n",
        "\n",
        "    else:\n",
        "        # 80/10/10 split for data\n",
        "        idxs = torch.randperm(len(dataset)).tolist()\n",
        "        one_tenth = len(dataset) // 10\n",
        "        training_data = Subset(\n",
        "            dataset, idxs[:one_tenth * 8])\n",
        "        training_data.dataset.transform = train_transform\n",
        "        validation_data = Subset(\n",
        "            dataset, idxs[one_tenth * 8:one_tenth * 9])\n",
        "        validation_data.dataset.transform = test_transform\n",
        "        test_data = Subset(dataset, idxs[one_tenth * 9:])\n",
        "        test_data.dataset.transform = test_transform\n",
        "\n",
        "        training_data = DataLoader(\n",
        "            dataset=training_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "        validation_data = DataLoader(\n",
        "            dataset=validation_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "        test_data = DataLoader(\n",
        "            dataset=test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        dls[0] = (training_data, validation_data, test_data)\n",
        "\n",
        "    return dls\n"
      ],
      "metadata": {
        "id": "QaRmSJ5CS7LD"
      },
      "id": "QaRmSJ5CS7LD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_save_dir(dir: str | PathLike, num_epochs: int, batch_size: int, learning_rate: float):\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "    cts = time.localtime()\n",
        "    name = f\"{cts[0]:02d}{cts[1]:02d}{cts[2]:02d}_{cts[3]:02d}{cts[4]:02d}{cts[5]:02d}_e{num_epochs}_b{batch_size}_lr{learning_rate}\"\n",
        "    save_dir = os.path.join(dir, name)\n",
        "    os.mkdir(save_dir)\n",
        "    return save_dir\n",
        "\n",
        "\n",
        "def get_model_name(num_epochs: int, batch_size: int, curr_epoch: int, learning_rate: float, fold: int):\n",
        "    return f\"{batch_size}b_{curr_epoch}_of_{num_epochs}e_{learning_rate}_f{fold}\"\n",
        "\n",
        "\n",
        "def get_model_dir(save_dir: str | PathLike, model_name: str):\n",
        "    return os.path.join(save_dir, \"models\",\n",
        "                        f\"{model_name}.pt\")\n",
        "\n",
        "\n",
        "def save_model(model: torch.nn.Module, save_dir: str | PathLike, model_name: str, curr_epoch: int, optimizer: Optimizer, scheduler: LRScheduler, top_5_mAPs: list[tuple[str, float]]):\n",
        "    if not os.path.exists(os.path.join(save_dir, \"models\")):\n",
        "        os.mkdir(os.path.join(save_dir, \"models\"))\n",
        "    torch.save(\n",
        "        {\n",
        "            \"epoch\": curr_epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer,\n",
        "            \"scheduler\": scheduler.state_dict(),\n",
        "            \"top_5_mAPs\": top_5_mAPs,\n",
        "        },\n",
        "        get_model_dir(save_dir, model_name)\n",
        "    )\n",
        "\n",
        "\n",
        "def remove_model(save_dir: str | PathLike, model_name: str):\n",
        "    os.remove(get_model_dir(save_dir, model_name))\n",
        "\n",
        "\n",
        "def save_results(save_dir: str | PathLike, trained_results: dict[int, dict], test_results: dict, args, *, dataloaders: dict[str, DataLoader] = None, best_models: list[tuple[str, float]] = None):\n",
        "    \"\"\"\n",
        "    Output the results from training and testing to the specified directory.\n",
        "    \"\"\"\n",
        "    with open(file=os.path.join(save_dir, \"readme.txt\"), mode=\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"This model has been trained with the following parameters:\\n\")\n",
        "        for arg in vars(args):\n",
        "            line = f\"\\t{arg}: {getattr(args, arg)}\\n\"\n",
        "            f.write(line)\n",
        "        if dataloaders:\n",
        "            for name, dataloader in dataloaders.items():\n",
        "                f.write(f\"{name} size: {len(dataloader)}\\n\")\n",
        "        if best_models:\n",
        "            f.write(\"Best models:\")\n",
        "            for item in best_models:\n",
        "                f.write(f\"\\t- Model: {item[0]} | mAP@0.5: {item[1]}\")\n",
        "    torch.save(trained_results, os.path.join(save_dir, \"trained_results.pt\"))\n",
        "    torch.save(test_results, os.path.join(save_dir, \"test_results.pt\"))\n",
        "\n",
        "######################\n",
        "#  Model  Functions  #\n",
        "######################\n",
        "\n",
        "\n",
        "def get_class_name(label: torch.Tensor):\n",
        "    return {1: \"privet\", 2: \"yew\", 3: \"path\"}[label.item()]\n",
        "\n",
        "\n",
        "def ref_train(model: torch.nn.Module, optimizer: Optimizer, train_data_loader: DataLoader, device, epoch):\n",
        "    result = train_one_epoch(\n",
        "        model, optimizer, train_data_loader, device, epoch, print_freq=10)\n",
        "    return result"
      ],
      "metadata": {
        "id": "8bk5REHoS_Cs"
      },
      "id": "8bk5REHoS_Cs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "#   Main Functions   #\n",
        "######################\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        prog=\"multifrequency_loader.py\",\n",
        "        description=\"Converts separate images with multiple frequencies into single tensor files.\",\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-m\", \"--model\", help=\"Which model to run.\\nOptions: {faster_rcnn, }\")\n",
        "    parser.add_argument(\"-c\", \"--channels\",\n",
        "                        help=\"Which channels to use.\\nOptions: {rgb, all}\")\n",
        "    parser.add_argument(\"--img_dir\", help=\"The outer image directory\")\n",
        "    parser.add_argument(\"--labels_dir\", help=\"The outer label directory\")\n",
        "    parser.add_argument(\n",
        "        \"--results_dir\", help=\"The directory to place all results\")\n",
        "    parser.add_argument(\"-e\", \"--num_epochs\", type=int, nargs=\"+\", default=[NUM_EPOCHS],\n",
        "                        help=\"The number of epochs, space-separated\")\n",
        "    parser.add_argument(\"-bs\", \"--batch_size\", type=int, nargs=\"+\", default=[BATCH_SIZE],\n",
        "                        help=\"All batch sizes to use, space-separated\")\n",
        "    parser.add_argument(\"-lr\", \"--learning_rate\", type=float, nargs=\"+\", default=[LEARNING_RATE],\n",
        "                        help=\"All learning rates to use, space-separated\")\n",
        "    parser.add_argument(\"--scheduler_step_size\", type=float,\n",
        "                        default=SCHEDULER_STEP_SIZE, help=\"Scheduler step size\")\n",
        "    parser.add_argument(\"--scheduler_gamma\", type=float,\n",
        "                        default=SCHEDULER_GAMMA, help=\"Scheduler gamma\")\n",
        "    parser.add_argument(\"--optimizer_momentum\", type=float,\n",
        "                        default=OPTIM_MOMENTUM, help=\"Optimizer momentum\")\n",
        "    parser.add_argument(\"--optimizer_weight_decay\", type=float,\n",
        "                        default=OPTIM_WEIGHT_DECAY, help=\"Optimizer weight decay\")\n",
        "    parser.add_argument(\"--kfold\", type=int, default=1)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args"
      ],
      "metadata": {
        "id": "05pF7g2kTDq0"
      },
      "id": "05pF7g2kTDq0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Starting...\")\n",
        "    args = parse_args()\n",
        "\n",
        "    # set up device\n",
        "    device = torch.accelerator.current_accelerator().type \\\n",
        "        if torch.accelerator.is_available() else \"cpu\"\n",
        "    print(f\"Using {device} device\")\n",
        "\n",
        "    set_seeds()\n",
        "\n",
        "    num_folds = args.kfold\n",
        "\n",
        "    for batch_size in args.batch_size:\n",
        "        for num_epochs in args.num_epochs:\n",
        "            for learning_rate in args.learning_rate:\n",
        "                # set up model\n",
        "                num_channels = 3\n",
        "                if args.channels == \"all\":\n",
        "                    num_channels = 14\n",
        "                model = get_model(args.model, num_channels=num_channels)\n",
        "                # print(model)\n",
        "                model.to(device)\n",
        "\n",
        "                # set up data\n",
        "                batch_size = batch_size\n",
        "                dataloaders = get_data(\n",
        "                    img_dir=args.img_dir, labels_dir=args.labels_dir, channels=args.channels, batch_size=batch_size, num_folds=num_folds)\n",
        "\n",
        "                trained_results = {}\n",
        "                eval_results = {}\n",
        "                top_5_mAPs: dict[list[tuple[str, float]]] = {}\n",
        "                for fold in range(num_folds):\n",
        "                    trained_results[fold] = {}\n",
        "                    eval_results[fold] = {}\n",
        "                    top_5_mAPs[fold] = []\n",
        "\n",
        "                for fold, (train_data, validation_data, test_data) in enumerate(dataloaders.values()):\n",
        "                    print(f\"Starting Fold {fold}\")\n",
        "\n",
        "                    # construct an optimizer\n",
        "                    params = [p for p in model.parameters() if p.requires_grad]\n",
        "                    optimizer = SGD(\n",
        "                        params,\n",
        "                        lr=learning_rate,\n",
        "                        momentum=args.optimizer_momentum,\n",
        "                        weight_decay=args.optimizer_weight_decay\n",
        "                    )\n",
        "\n",
        "                    # and a learning rate scheduler\n",
        "                    lr_scheduler = StepLR(\n",
        "                        optimizer,\n",
        "                        step_size=args.scheduler_step_size,\n",
        "                        gamma=args.scheduler_gamma\n",
        "                    )\n",
        "\n",
        "                    save_dir = get_save_dir(\n",
        "                        dir=args.results_dir, num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate)\n",
        "\n",
        "\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "                        trained_results[fold][epoch] = ref_train(\n",
        "                            model=model, optimizer=optimizer, train_data_loader=train_data, device=device, epoch=epoch)\n",
        "                        lr_scheduler.step()\n",
        "\n",
        "                        # evaluate on the validation dataset\n",
        "                        validation_result = evaluate(\n",
        "                            model, validation_data, device=device)\n",
        "                        eval_results[fold][epoch] = validation_result\n",
        "\n",
        "                        mAP = validation_result.coco_eval['bbox'].stats[1]\n",
        "\n",
        "                        # save results\n",
        "                        save_results(save_dir=save_dir,\n",
        "                                     trained_results=trained_results, test_results=eval_results, args=args)\n",
        "\n",
        "                        # save model if in top 5\n",
        "                        model_name = get_model_name(num_epochs=num_epochs, batch_size=batch_size, curr_epoch=epoch, learning_rate=learning_rate, fold=fold)\n",
        "                        if len(top_5_mAPs[fold]) < 5:\n",
        "                            save_model(model=model, save_dir=save_dir, model_name=model_name, curr_epoch=epoch,\n",
        "                                       optimizer=optimizer, scheduler=lr_scheduler, top_5_mAPs=top_5_mAPs)\n",
        "                            top_5_mAPs[fold].append((model_name, mAP))\n",
        "                            top_5_mAPs[fold].sort(key=itemgetter(1), reverse=True)\n",
        "                        elif mAP > top_5_mAPs[fold][-1][1]:\n",
        "                            for i in range(len(top_5_mAPs[fold])):\n",
        "                                if mAP > top_5_mAPs[fold][i][1]:\n",
        "                                    top_5_mAPs[fold].insert(i, (model_name, mAP))\n",
        "                                    break\n",
        "                            name_to_delete = top_5_mAPs[fold][-1][0]\n",
        "                            try:\n",
        "                                os.remove(get_model_dir(\n",
        "                                    save_dir, name_to_delete))\n",
        "                            except:\n",
        "                                continue\n",
        "                            save_model(model=model, save_dir=save_dir, model_name=model_name, curr_epoch=epoch,\n",
        "                                       optimizer=optimizer, scheduler=lr_scheduler, top_5_mAPs=top_5_mAPs)\n",
        "                            top_5_mAPs[fold] = top_5_mAPs[fold][:-1]\n",
        "\n",
        "                    print(\"\\n\\nTraining complete!\\n\\n\")\n",
        "\n",
        "                    # test\n",
        "                    eval_results[fold][-1] = evaluate(model,\n",
        "                                                test_data, device=device)\n",
        "                    save_results(save_dir=save_dir,\n",
        "                                 trained_results=trained_results, test_results=eval_results, args=args, dataloaders={\"training_data\": train_data, \"validation_data\": validation_data, \"testing_data\": test_data}, best_models=top_5_mAPs)\n",
        "\n",
        "                    total_time = time.time() - start_time\n",
        "                    print(f\"Entire run took {total_time}s\")\n",
        "\n",
        "                    # make_graphs_and_vis(save_dir=save_dir,\n",
        "                                        # trained_results=trained_results, test_results=eval_results, best_models=top_5_mAPs, test_data=test_data)\n",
        "\n",
        "                print(\"Complete!\")\n"
      ],
      "metadata": {
        "id": "vhjFLfplTHBT"
      },
      "id": "vhjFLfplTHBT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "t7vIkMyOTKFU"
      },
      "id": "t7vIkMyOTKFU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python privet_detection/src/main.ipynb -m faster_rcnn --img_dir \"/content/drive/MyDrive/Privet/data/Pruned Data for Training/images\" --labels_dir \"/content/drive/MyDrive/Privet/data/Pruned Data for Training/labels\" --results_dir \"\" --num_epochs 100 -bs 2 4 -lr 0.1 0.01 0.001 --kfold 5"
      ],
      "metadata": {
        "id": "YS-dsqqQTM0r",
        "outputId": "e1ccee93-9b10-4375-e6ed-10e0beb06869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YS-dsqqQTM0r",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/privet_detection/src/main.ipynb\", line 5, in <module>\n",
            "    \"execution_count\": null,\n",
            "                       ^^^^\n",
            "NameError: name 'null' is not defined\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}